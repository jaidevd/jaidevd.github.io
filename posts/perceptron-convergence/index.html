<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=format-detection content="telephone=no"><title>A Geometric Proof of the Perceptron Convergence Theorem | Jaidev's Blog</title><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#ff3db4><meta name=theme-color content="#ffffff"><link rel=stylesheet href=http://jaidevd.com/css/main.min.975b1911c008aee6ab5fb42e51274b8268ebcb65dc15bd4a5f69b9eedb485c3e.css><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-80129098-1','auto'),ga('send','pageview'))</script></head><body><nav><header><div class=site-title><a href=/>Jaidev's Blog</a></div></header><div class=nav-menu><a class="color-link nav-link" href=/about/>About Me</a>
<a class="color-link nav-link" href=/archive/>Archive</a>
<a class="color-link nav-link" href=http://jaidevd.com/index.xml target=_blank rel=noopener type=application/rss+xml>RSS</a></div><footer class=footer><div class=social-icons><a class=social-icon href=mailto:deshpande.jaidev@gmail.com target=_blank rel=noopener title=Email><svg width="28" height="28" viewBox="0 0 28 28" fill="#ababab" xmlns="https://www.w3.org/2000/svg" xmlns:xlink="https://www.w3.org/1999/xlink"><path d="M25.2794292 5.59128519 14 16.8707144 2.72057081 5.59128519C3.06733103 5.30237414 3.51336915 5.12857603 4 5.12857603H24c.4866308.0.932669000000001.173798110000001 1.2794292.46270916zM25.9956978 6.99633695C25.998551 7.04004843 26 7.08414302 26 7.12857603V20.871424C26 21.0798433 25.9681197 21.2808166 25.9089697 21.4697335l-7.1933342-7.1933342 7.2800623-7.28006235zM24.9498374 22.6319215C24.6672737 22.7846939 24.3437653 22.871424 24 22.871424H4C3.5268522 22.871424 3.09207889 22.7071233 2.74962118 22.432463l7.34540352-7.3454036 3.8897821 3.8897821L14.1878486 18.7737996l.0151933.0151933 3.4519334-3.4519335 7.2948621 7.2948621zM2.00810114 21.0526627C2.00273908 20.9929669 2 20.9325153 2 20.871424V7.12857603C2 7.08414302 2.00144896 7.04004843 2.00430222 6.99633695L9.03436454 14.0263993l-7.0262634 7.0262634z"/></svg></a><a class=social-icon href=https://twitter.com/jaidevd target=_blank rel=noopener title=Twitter><svg width="28" height="28" viewBox="0 0 28 28" fill="#ababab" xmlns="https://www.w3.org/2000/svg" xmlns:xlink="https://www.w3.org/1999/xlink"><path d="M8.991284 24.971612c10.189152.0 15.761088-8.441388 15.761088-15.761088C24.752372 8.970656 24.747512 8.731868 24.736496 8.494376 25.818008 7.712564 26.758256 6.737 27.5 5.62622c-.992628.440856-2.060748.738072-3.181248.871992 1.14372-.685584 2.02176-1.770768 2.435832-3.064176-1.070496.6345-2.25558 1.095984-3.517344 1.344492-1.010772-1.076652-2.450412-1.75014-4.043412-1.75014-3.059424.0-5.540292 2.480868-5.540292 5.539104.0.434808.0487079999999995.857412.14364 1.26306C9.19346 9.599108 5.11106 7.39472 2.3792 4.04294c-.476172.818424-.750168 1.769688-.750168 2.784132.0 1.921968.97794 3.61854 2.464992 4.61106C3.185528 11.41016 2.331788 11.160464 1.585184 10.745096 1.583888 10.768208 1.583888 10.791428 1.583888 10.815728c0 2.683152 1.909764 4.922856 4.4442 5.43078C5.562932 16.373084 5.07326 16.44134 4.56782 16.44134 4.210988 16.44134 3.863876 16.406024 3.526484 16.34144c.70524 2.200824 2.750112 3.802356 5.174928 3.8475-1.896264 1.485756-4.284576 2.37114-6.879924 2.37114C1.374476 22.56008.93362 22.534592.5 22.4834c2.451708 1.571076 5.362524 2.488212 8.491284 2.488212"/></svg></a><a class=social-icon href="https://www.youtube.com/playlist?list=PLllKLgiXxcqe3MlAk-6ZrQP82Dr5mgI0d" target=_blank rel=noopener title=YouTube><svg width="28" height="28" viewBox="0 0 28 28" fill="#ababab" xmlns="https://www.w3.org/2000/svg" xmlns:xlink="https://www.w3.org/1999/xlink"><path d="M25.9775568 20.4086648C25.6900568 21.4913352 24.8430398 22.343892 23.7673295 22.6332386 21.8177557 23.1590909 14 23.1590909 14 23.1590909s-7.81771307.0-9.76734375-.5258523C3.15704545 22.343892 2.30988068 21.4913352 2.02240483 20.4086648 1.5 18.4464062 1.5 14.3522727 1.5 14.3522727s0-4.0940767.52240483-6.05651986c.28747585-1.08254261 1.13464062-1.93509091 2.21025142-2.22456392C6.18228693 5.54545455 14 5.54545455 14 5.54545455s7.8177557.0 9.7673295.52573437c1.0757103.28947301 1.9227273 1.14202131 2.2102273 2.22456392C26.5 10.258196 26.5 14.3522727 26.5 14.3522727S26.5 18.4464062 25.9775568 20.4086648zm-14.534375-9.773537v7.434304l6.5340909-3.7173295-6.5340909-3.7169745z"/></svg></a><a class=social-icon href=https://github.com/jaidevd target=_blank rel=noopener title=GitHub><svg width="28" height="28" viewBox="0 0 28 28" fill="#ababab" xmlns="https://www.w3.org/2000/svg" xmlns:xlink="https://www.w3.org/1999/xlink"><path d="M13.9988029 1.32087331C6.82105037 1.32087331 1 7.14112562 1 14.3212723c0 5.7436386 3.72454649 10.6157955 8.89038951 12.3348169C10.5408085 26.7757983 10.7778323 26.374374 10.7778323 26.0296121 10.7778323 25.7215609 10.7666595 24.9035493 10.760275 23.8189856 7.14426471 24.6042767 6.38131925 22.0760223 6.38131925 22.0760223c-.59136253-1.5019491-1.44369072-1.9017772-1.44369072-1.9017772C3.75729765 19.3682044 5.02701126 19.3841656 5.02701126 19.3841656 6.33183953 19.4759425 7.01817121 20.7241085 7.01817121 20.7241085c1.15958133 1.9863716 3.04300319 1.4125664 3.78360289 1.0797753.1181129-.8395592.4540962-1.4125663.8251942-1.7373768C8.74038491 19.7385043 5.70536235 18.6228163 5.70536235 13.6413251c0-1.4189508.50676816-2.5801283 1.33834679-3.4883207C6.90963504 9.82420367 6.46351945 8.50181809 7.17139875 6.71256734c0 0 1.09094816-.34955032 3.57451115 1.33276037C11.78259 7.75642995 12.8950858 7.61277914 14.000399 7.60719272 15.1049142 7.61277914 16.2166119 7.75642995 17.2548881 8.04532771c2.4819669-1.68231069 3.571319-1.33276037 3.571319-1.33276037C21.5356825 8.50181809 21.0895669 9.82420367 20.9562909 10.1530044 21.7894656 11.0611968 22.2922435 12.2223743 22.2922435 13.6413251c0 4.9942601-3.039811 6.0931889-5.935173 6.4148071.4660671.401424200000001.8818564 1.194696.8818564 2.4077473C17.2389269 24.2012564 17.2229657 25.603448 17.2229657 26.0296121 17.2229657 26.3775663 17.4575954 26.7821827 18.116793 26.6552912 23.2786458 24.9322794 27 20.0633148 27 14.3212723 27 7.14112562 21.1789496 1.32087331 13.9988029 1.32087331"/></svg></a></div><p><a href=https://github.com/kimcc/hugo-theme-noteworthy target=_blank rel=noopener>Noteworthy theme</a></p><p><a href=https://gohugo.io target=_blank rel=noopener>Built with Hugo</a></p><script src=http://jaidevd.com/js/main.min.c1aee25a817e9beb1f9c4afd9d62311227a7f5e46720e404dc1dda97281f47f2.js integrity="sha256-wa7iWoF+m+sfnEr9nWIxEien9eRnIOQE3B3alygfR/I=" crossorigin=anonymous></script></footer></nav><div id=content class=content-container><h1 class=post-title>A Geometric Proof of the Perceptron Convergence Theorem</h1><time>July 6, 2016</time><div><p><p>The last time I studied neural networks in detail was five years ago in college. I did touch upon backpropagation when Andrew Ng&rsquo;s machine learning MOOC was offered on Coursera for the first time, but beyond that I&rsquo;ve only dabbled with them through <a href=http://keras.io>keras</a>. Then recently, when I read about Coursera&rsquo;s imminent decision to pull down much of their freely available material (you can read a rant about it <a href=http://reachtarunhere.github.io/2016/06/11/Golden-Age-of-MOOCs-is-over-and-why-I-hate-Coursera/>here</a>), I went on a downloading spree (many thanks to the wonderful <a href=http://github.com/coursera-dl/coursera-dl>coursera-dl</a>). Of all the courses I downloaded, the one that caught my eye was Geoffrey Hinton&rsquo;s course on <em>Neural Networks for Machine Learning</em>. Because of that and the fact that there were some computer vision projects going on at work, I decided to dive right in.</p><p>Hinton&rsquo;s course is wonderful. He is funny, and unsurprisingly, very very insightful about the core concepts in neural networks. One of the signs of this is the fact that this course is not at all cluttered with too much mathematics, and can be traveresed by someone with only a working knowledge of calculus. One of his most insightful moments in the course is when he describes the Perceptron learning rule as simply as follows:</p><ul><li>If the perceptron makes no mistake, leave it alone.</li><li>If it predicts a false negative, add the input vector to the weight vector</li><li>If it predicts a false positive, subtract the input vector from the weight vector</li></ul><p>This is so simple, that a literal implementation of this can make train a perceptron reasonably well (as we shall see). There are of course, numerous heuristics required when applying it in production, but the training algorithm is just this simple. Now, the popularity of the perceptron is because it guarantees linear convergence, i.e. if a binary classification problem is linearly separable in the feature space, the perceptron will <em>always eventually</em> learn to correctly classify the input samples. An algebraic or analytical proof of this can be found anywhere, but relies almost always on the <a href=https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality>Cauchy-Schwarz inequality</a>. I thought that since the learning rule is so simple, then there must be a way to understand the convergence theorem using nothing more than the learning rule itself, and some simple data visualization. I think I&rsquo;ve found a reasonable explanation, which is what this post is broadly about. But first, let&rsquo;s see a simple demonstration of training a perceptron.</p><h2 id=a-demo-perceptron>A Demo Perceptron</h2><p>Let&rsquo;s make some sample data which is linearly separable. Let&rsquo;s also initialize the weights for a perceptron. For simplicity, we shall do away with the bias, and make sure that the data is centered at the origin</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#000;font-weight:700>import</span> <span style=color:#555>numpy</span> <span style=color:#000;font-weight:700>as</span> <span style=color:#555>np</span>
<span style=color:#000;font-weight:700>from</span> <span style=color:#555>sklearn.datasets</span> <span style=color:#000;font-weight:700>import</span> make_blobs
<span style=color:#000;font-weight:700>import</span> <span style=color:#555>matplotlib.pyplot</span> <span style=color:#000;font-weight:700>as</span> <span style=color:#555>plt</span>

X, y <span style=color:#000;font-weight:700>=</span> make_blobs(centers<span style=color:#000;font-weight:700>=</span>[[<span style=color:#099>1</span>, <span style=color:#099>1</span>], [<span style=color:#000;font-weight:700>-</span><span style=color:#099>1</span>, <span style=color:#000;font-weight:700>-</span><span style=color:#099>1</span>]], cluster_std<span style=color:#000;font-weight:700>=</span><span style=color:#099>0.5</span>)
np<span style=color:#000;font-weight:700>.</span>random<span style=color:#000;font-weight:700>.</span>seed(<span style=color:#099>3</span>)
weights <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>array([[<span style=color:#000;font-weight:700>-</span><span style=color:#099>1</span>], [<span style=color:#099>1</span>]], dtype<span style=color:#000;font-weight:700>=</span><span style=color:#0086b3>float</span>)</code></pre></div><p>Let&rsquo;s also define a function that makes a scatterplot of the data and draws the decision boundary through it, based on the weights. The decision function we use is as follows</p><p>$$z = \sum_{i=1}^{N}w_{i}x_{i}$$</p><p>Where $N$ is the dimensionality, $x_{i}$ is the $i$th dimension of the input sample, and $w_{i}$ is the corresponding weight.</p><p>The prediction $y$ is 1 if $z \geq 0$ and 0 otherwise. Thus, the decision line in the feature space (consisting in this case of $x_{1}$ and $x_{2}$) is defined as follows:</p><p>$$ w_{1}x_{1} + w_{2}x_{2} = 0 $$</p><p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>plot_decision</span>(X, y, weights):
    <span style=color:#d14>&#34;&#34;&#34;Show the scatterplot of the data colored by the classes,
</span><span style=color:#d14>    draw the decision line based on the weights.&#34;&#34;&#34;</span>
    xx <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>linspace(X[:, <span style=color:#099>0</span>]<span style=color:#000;font-weight:700>.</span>min(), X[:, <span style=color:#099>0</span>]<span style=color:#000;font-weight:700>.</span>max())
    yy <span style=color:#000;font-weight:700>=</span> <span style=color:#000;font-weight:700>-</span> weights[<span style=color:#099>0</span>] <span style=color:#000;font-weight:700>*</span> xx <span style=color:#000;font-weight:700>/</span> weights[<span style=color:#099>1</span>]
    plt<span style=color:#000;font-weight:700>.</span>figure(figsize<span style=color:#000;font-weight:700>=</span>(<span style=color:#099>8</span>, <span style=color:#099>6</span>))
    plt<span style=color:#000;font-weight:700>.</span>scatter(X[:, <span style=color:#099>0</span>], X[:, <span style=color:#099>1</span>], c<span style=color:#000;font-weight:700>=</span>y)
    plt<span style=color:#000;font-weight:700>.</span>plot(xx, yy)

plot_decision(X, y, weights)</code></pre></div><img src=/img/perc-1.png alt></p><p>Note that I&rsquo;ve specifically chosen the weights such that the decision boundary ends up in the worst possible position. Now let&rsquo;s train the perceptron in batch mode on one epoch of the training set.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>prediction <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>dot(X, weights)<span style=color:#000;font-weight:700>.</span>ravel()<span style=color:#000;font-weight:700>.</span>astype(<span style=color:#0086b3>int</span>)
<span style=color:#998;font-style:italic># indentify the misclassifications</span>
false_pos <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>logical_and(y <span style=color:#000;font-weight:700>==</span> <span style=color:#099>0</span>, prediction <span style=color:#000;font-weight:700>==</span> <span style=color:#099>1</span>)
false_neg <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>logical_and(y <span style=color:#000;font-weight:700>==</span> <span style=color:#099>1</span>, prediction <span style=color:#000;font-weight:700>==</span> <span style=color:#099>0</span>)
<span style=color:#998;font-style:italic># add the false negatives</span>
weights <span style=color:#000;font-weight:700>+=</span> X[false_neg, :]<span style=color:#000;font-weight:700>.</span>sum(<span style=color:#099>0</span>)<span style=color:#000;font-weight:700>.</span>reshape(<span style=color:#000;font-weight:700>-</span><span style=color:#099>1</span>, <span style=color:#099>1</span>)
<span style=color:#998;font-style:italic># subtract the false positives</span>
weights <span style=color:#000;font-weight:700>-=</span> X[false_pos, :]<span style=color:#000;font-weight:700>.</span>sum(<span style=color:#099>0</span>)<span style=color:#000;font-weight:700>.</span>reshape(<span style=color:#000;font-weight:700>-</span><span style=color:#099>1</span>, <span style=color:#099>1</span>)

<span style=color:#998;font-style:italic># plot the data again</span>
plot_decision(X, y, weights)</code></pre></div><p><img src=/img/perc-2.png alt></p><p>It can be seen that one epoch or a hundred iterations (maybe less) are enough to train the perceptron. Now let&rsquo;s do the same in online mode. This time, we will store the weights updated at each iteration so we can have a look at them later.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#998;font-style:italic># reset the weights</span>
weights <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>array([[<span style=color:#000;font-weight:700>-</span><span style=color:#099>1</span>], [<span style=color:#099>1</span>]], dtype<span style=color:#000;font-weight:700>=</span><span style=color:#0086b3>float</span>)
updated_weights <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>zeros((<span style=color:#099>100</span>, <span style=color:#099>2</span>))
<span style=color:#000;font-weight:700>for</span> i <span style=color:#000;font-weight:700>in</span> <span style=color:#0086b3>range</span>(X<span style=color:#000;font-weight:700>.</span>shape[<span style=color:#099>0</span>]):
    updated_weights[i, :] <span style=color:#000;font-weight:700>=</span> weights<span style=color:#000;font-weight:700>.</span>ravel()  <span style=color:#998;font-style:italic># store the weights</span>
    training_sample <span style=color:#000;font-weight:700>=</span> X[i, :]
    target <span style=color:#000;font-weight:700>=</span> y[i]
    prediction <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>dot(training_sample, weights)<span style=color:#000;font-weight:700>.</span>ravel()<span style=color:#000;font-weight:700>.</span>astype(<span style=color:#0086b3>int</span>)
    <span style=color:#000;font-weight:700>if</span> target <span style=color:#000;font-weight:700>!=</span> prediction:
        <span style=color:#000;font-weight:700>if</span> target <span style=color:#000;font-weight:700>==</span> <span style=color:#099>0</span> <span style=color:#000;font-weight:700>and</span> prediction <span style=color:#000;font-weight:700>==</span> <span style=color:#099>1</span>:  <span style=color:#998;font-style:italic># false positive</span>
            weights <span style=color:#000;font-weight:700>-=</span> training_sample<span style=color:#000;font-weight:700>.</span>reshape(<span style=color:#000;font-weight:700>-</span><span style=color:#099>1</span>, <span style=color:#099>1</span>)
        <span style=color:#000;font-weight:700>elif</span> target <span style=color:#000;font-weight:700>==</span> <span style=color:#099>1</span> <span style=color:#000;font-weight:700>and</span> prediction <span style=color:#000;font-weight:700>==</span> <span style=color:#099>0</span>:  <span style=color:#998;font-style:italic># false negative</span>
            weights <span style=color:#000;font-weight:700>+=</span> training_sample<span style=color:#000;font-weight:700>.</span>reshape(<span style=color:#000;font-weight:700>-</span><span style=color:#099>1</span>, <span style=color:#099>1</span>)</code></pre></div><p>The two snippets above achieve exactly the same effect, except that the first is faster since it is vectorized. Now that we have the weight updates for every training sample, let&rsquo;s plot the decision boundary for a few of them.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>sample_ix <span style=color:#000;font-weight:700>=</span> [<span style=color:#099>1</span>, <span style=color:#099>20</span>, <span style=color:#099>30</span>]
xx <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>linspace(X[:, <span style=color:#099>0</span>]<span style=color:#000;font-weight:700>.</span>min(), X[:, <span style=color:#099>0</span>]<span style=color:#000;font-weight:700>.</span>max(), <span style=color:#099>100</span>)
plt<span style=color:#000;font-weight:700>.</span>figure(figsize<span style=color:#000;font-weight:700>=</span>(<span style=color:#099>8</span>, <span style=color:#099>6</span>))
plt<span style=color:#000;font-weight:700>.</span>scatter(X[:, <span style=color:#099>0</span>], X[:, <span style=color:#099>1</span>], c<span style=color:#000;font-weight:700>=</span>y)

<span style=color:#000;font-weight:700>for</span> ix <span style=color:#000;font-weight:700>in</span> sample_ix:
    wghts <span style=color:#000;font-weight:700>=</span> updated_weights[ix, :]
    yy <span style=color:#000;font-weight:700>=</span> <span style=color:#000;font-weight:700>-</span> wghts[<span style=color:#099>0</span>] <span style=color:#000;font-weight:700>*</span> xx <span style=color:#000;font-weight:700>/</span> wghts[<span style=color:#099>1</span>]
    plt<span style=color:#000;font-weight:700>.</span>plot(xx, yy, label<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#34;{}th sample&#34;</span><span style=color:#000;font-weight:700>.</span>format(ix))
plt<span style=color:#000;font-weight:700>.</span>legend()</code></pre></div><p><img src=/img/perc-3.png alt></p><p>As we move from the first to the twentieth and then to the thirtieth sample, we can see that the decision boundary is rotating counter clockwise. By the thirtieth sample, it has found the gap that separates the two blobs, so it stops making mistakes, and therefore stops learning. Now let&rsquo;s try and see why this will <em>always</em> happen, as long as there is such a gap to be found, irrespective of the topology of the data or the number of points in the dataset.</p><h2 id=the-proof>The Proof</h2><p>We shall continue, for the sake of simplicity, our assumption of the data being centered at the origin and therefore not needing a bias term in the learning. For better visualization, we will also impose a restriction on the weight vector - that it needs to have a unit norm. Note that the magnitude of the weight vector does not matter, since it is only its gradient that actually affects the predictions, and therefore we can comfortably impose this restriction. Moreover, I&rsquo;m insisting on this restriction only to make the plots look pretty.</p><p>Consider the following dataset:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>X <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>array([[<span style=color:#000;font-weight:700>-</span><span style=color:#099>0.5</span>, <span style=color:#099>1</span>],
              [<span style=color:#000;font-weight:700>-</span><span style=color:#099>1</span>, <span style=color:#000;font-weight:700>-</span><span style=color:#099>0.2</span>],
              [<span style=color:#099>0.5</span>, <span style=color:#000;font-weight:700>-</span><span style=color:#099>1</span>],
              [<span style=color:#099>1</span>, <span style=color:#000;font-weight:700>-</span><span style=color:#099>0.2</span>]])
y <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>array([<span style=color:#099>1</span>, <span style=color:#099>1</span>, <span style=color:#099>0</span>, <span style=color:#099>0</span>])
weights <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>array([<span style=color:#099>0.707</span>, <span style=color:#099>0.707</span>])</code></pre></div><p>Now, we arrange the training samples and the weights in a cartesian plane, by their position vectors.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>cls_color <span style=color:#000;font-weight:700>=</span> [<span style=color:#d14>&#34;r&#34;</span>, <span style=color:#d14>&#34;g&#34;</span>]
plt<span style=color:#000;font-weight:700>.</span>figure(figsize<span style=color:#000;font-weight:700>=</span>(<span style=color:#099>6</span>, <span style=color:#099>6</span>))
ax <span style=color:#000;font-weight:700>=</span> plt<span style=color:#000;font-weight:700>.</span>axes()
ax<span style=color:#000;font-weight:700>.</span>set_xlim(<span style=color:#000;font-weight:700>-</span><span style=color:#099>2</span>, <span style=color:#099>2</span>)
ax<span style=color:#000;font-weight:700>.</span>set_ylim(<span style=color:#000;font-weight:700>-</span><span style=color:#099>2</span>, <span style=color:#099>2</span>)
<span style=color:#000;font-weight:700>for</span> i <span style=color:#000;font-weight:700>in</span> <span style=color:#0086b3>range</span>(<span style=color:#099>4</span>):
    ax<span style=color:#000;font-weight:700>.</span>arrow(<span style=color:#099>0</span>, <span style=color:#099>0</span>, X[i, <span style=color:#099>0</span>], X[i, <span style=color:#099>1</span>], fc<span style=color:#000;font-weight:700>=</span>cls_color[y[i]], ec<span style=color:#000;font-weight:700>=</span>cls_color[y[i]],
    	     head_length<span style=color:#000;font-weight:700>=</span><span style=color:#099>0.1</span>, head_width<span style=color:#000;font-weight:700>=</span><span style=color:#099>0.1</span>)
ax<span style=color:#000;font-weight:700>.</span>arrow(<span style=color:#099>0</span>, <span style=color:#099>0</span>, weights[<span style=color:#099>0</span>], weights[<span style=color:#099>1</span>], fc<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#34;k&#34;</span>, ec<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#34;k&#34;</span>, head_width<span style=color:#000;font-weight:700>=</span><span style=color:#099>0.1</span>,
         head_length<span style=color:#000;font-weight:700>=</span><span style=color:#099>0.1</span>)

<span style=color:#998;font-style:italic># decision boundary:</span>
xx <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>linspace(<span style=color:#000;font-weight:700>-</span><span style=color:#099>2</span>, <span style=color:#099>2</span>, <span style=color:#099>100</span>)
yy <span style=color:#000;font-weight:700>=</span> <span style=color:#000;font-weight:700>-</span>weights[<span style=color:#099>0</span>] <span style=color:#000;font-weight:700>*</span> xx <span style=color:#000;font-weight:700>/</span> weights[<span style=color:#099>1</span>]
ax<span style=color:#000;font-weight:700>.</span>plot(xx, yy, <span style=color:#d14>&#34;k--&#34;</span>)</code></pre></div><p><img src=/img/perc-4.png alt></p><p>The green arrows denote the true positives, the red ones denote the true negatives, the black arrow is the weight vector and the black dashed line denotes the decision boundary. Our hypothesis based on this decision boundary is equivalent to saying that any point in the upper right triangle would be assigned a positive prediction, and any point in the lower left triangle would be assigned a negative prediction. In the given dataset, this hypothesis clearly makes two mistakes - one false positive and one false negative.</p><p>Analogous to the previous example, we know that the perceptron will converge if the weight vector (and hence the decision boundary) keeps rotating. Let&rsquo;s assume that it rotates counter-clockwise. Proving the convergence, then, can be reduced into proving the following parts:</p><ul><li><strong>Part A</strong>: No sample will ever make the weight vector rotate clockwise.</li><li><strong>Part B</strong>: The weight vector will not rotate enough to make the decision boundary jump <em>across</em> the gap between the two classes.</li></ul><p>Let us apply the learning rule to this perceptron, and see how we can prove both Part A and Part B. Since there is one false positive and one false negative, the training simply consists of adding the false negative to the weight vector and subtracting the false positive from it.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>fig, (ax1, ax2) <span style=color:#000;font-weight:700>=</span> plt<span style=color:#000;font-weight:700>.</span>subplots(nrows<span style=color:#000;font-weight:700>=</span><span style=color:#099>1</span>, ncols<span style=color:#000;font-weight:700>=</span><span style=color:#099>2</span>, figsize<span style=color:#000;font-weight:700>=</span>(<span style=color:#099>16</span>, <span style=color:#099>8</span>))

ax1<span style=color:#000;font-weight:700>.</span>axis([<span style=color:#000;font-weight:700>-</span><span style=color:#099>1.5</span>, <span style=color:#099>1.5</span>, <span style=color:#000;font-weight:700>-</span><span style=color:#099>1.5</span>, <span style=color:#099>1.5</span>])
ax2<span style=color:#000;font-weight:700>.</span>axis([<span style=color:#000;font-weight:700>-</span><span style=color:#099>1.5</span>, <span style=color:#099>1.5</span>, <span style=color:#000;font-weight:700>-</span><span style=color:#099>1.5</span>, <span style=color:#099>1.5</span>])
<span style=color:#998;font-style:italic># original weights</span>
weights <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>array([<span style=color:#099>0.707</span>, <span style=color:#099>0.707</span>])

<span style=color:#998;font-style:italic># adding the false negative and normalizing</span>
weights <span style=color:#000;font-weight:700>+=</span> [<span style=color:#000;font-weight:700>-</span><span style=color:#099>1</span>, <span style=color:#000;font-weight:700>-</span><span style=color:#099>0.2</span>]
weights <span style=color:#000;font-weight:700>=</span> weights <span style=color:#000;font-weight:700>/</span> np<span style=color:#000;font-weight:700>.</span>linalg<span style=color:#000;font-weight:700>.</span>norm(weights)
<span style=color:#000;font-weight:700>for</span> i <span style=color:#000;font-weight:700>in</span> <span style=color:#0086b3>range</span>(<span style=color:#099>4</span>):
    ax1<span style=color:#000;font-weight:700>.</span>arrow(<span style=color:#099>0</span>, <span style=color:#099>0</span>, X[i, <span style=color:#099>0</span>], X[i, <span style=color:#099>1</span>], fc<span style=color:#000;font-weight:700>=</span>cls_color[y[i]],
    	      ec<span style=color:#000;font-weight:700>=</span>cls_color[y[i]], head_length<span style=color:#000;font-weight:700>=</span><span style=color:#099>0.1</span>,
              head_width<span style=color:#000;font-weight:700>=</span><span style=color:#099>0.1</span>)
ax1<span style=color:#000;font-weight:700>.</span>arrow(<span style=color:#099>0</span>, <span style=color:#099>0</span>, weights[<span style=color:#099>0</span>], weights[<span style=color:#099>1</span>], fc<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#34;k&#34;</span>, ec<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#34;k&#34;</span>,
 	  head_width<span style=color:#000;font-weight:700>=</span><span style=color:#099>0.1</span>, head_length<span style=color:#000;font-weight:700>=</span><span style=color:#099>0.1</span>)
ax1<span style=color:#000;font-weight:700>.</span>set_title(<span style=color:#d14>&#34;After adding false -ve&#34;</span>)

<span style=color:#998;font-style:italic># subtracting the false positive and normalizing</span>
weights <span style=color:#000;font-weight:700>-=</span> [<span style=color:#099>1</span>, <span style=color:#000;font-weight:700>-</span><span style=color:#099>0.2</span>]
weights <span style=color:#000;font-weight:700>=</span> weights <span style=color:#000;font-weight:700>/</span> np<span style=color:#000;font-weight:700>.</span>linalg<span style=color:#000;font-weight:700>.</span>norm(weights)
<span style=color:#000;font-weight:700>for</span> i <span style=color:#000;font-weight:700>in</span> <span style=color:#0086b3>range</span>(<span style=color:#099>4</span>):
    ax2<span style=color:#000;font-weight:700>.</span>arrow(<span style=color:#099>0</span>, <span style=color:#099>0</span>, X[i, <span style=color:#099>0</span>], X[i, <span style=color:#099>1</span>], fc<span style=color:#000;font-weight:700>=</span>cls_color[y[i]], ec<span style=color:#000;font-weight:700>=</span>cls_color[y[i]],
              head_length<span style=color:#000;font-weight:700>=</span><span style=color:#099>0.1</span>,
              head_width<span style=color:#000;font-weight:700>=</span><span style=color:#099>0.1</span>)
ax2<span style=color:#000;font-weight:700>.</span>arrow(<span style=color:#099>0</span>, <span style=color:#099>0</span>, weights[<span style=color:#099>0</span>], weights[<span style=color:#099>1</span>], fc<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#34;k&#34;</span>, ec<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#34;k&#34;</span>,
	  head_width<span style=color:#000;font-weight:700>=</span><span style=color:#099>0.1</span>, head_length<span style=color:#000;font-weight:700>=</span><span style=color:#099>0.1</span>)
ax2<span style=color:#000;font-weight:700>.</span>set_title(<span style=color:#d14>&#34;After adding false -ve and subtracting false +ve&#34;</span>)</code></pre></div><p><img src=/img/perc-5.png alt></p><h2 id=part-a>Part A</h2><p>Suppose a false negative makes and angle $\theta_{n}$ with the weight vector. Since the decision boundary is orothogonal to the weight vector, no matter which iteration of learning the perceptron is, every possible $\theta_{n}$ will be such that $\frac{\pi}{2} \leq \theta_{n} \leq \frac{3\pi}{2}$. In order to make the weight vector rotate clockwise, a vector which makes an angle of $\leq \frac{\pi}{2}$ must be added to it. But because of the orthogonality between the weight vector and the decision boundary, no false negative can have this property! Thus, there will never exist a false negative that makes the weight vector rotate clockwise.</p><p>In the same way, suppose a false positive makes an angle $\theta_{p}$ with the weight vector. Again, every possible $\theta_{p}$ will be such that $-\frac{\pi}{2} \leq \theta_{p} \leq \frac{\pi}{2}$. Here, too, there can&rsquo;t exist a vector which when subtracted from the weight vector will make it rotate clockwise. Thus, there will never exist a false positive that makes the weight vector rotate clockwise.</p><p>In the context of the example, the only way that the weight vector could rotate clockwise was if there was a green vector sticking between the two red vectors, or vice versa. In both these cases, linear separability would not be possible (remember that we are assuming no bias), and hence the convergence would fail by definition.</p><h2 id=part-b>Part B</h2><p>Recall that in order to make corrections, what we are adding to (or subtracting from) the weights are the sample vectors themselves - not some linear function of them. In other words, whenever the perceptron learns, it learns not only to be right, but to be right by a very specific margin. This margin is nothing but the magnitude of the last mistake it made! Now, the only way that this margin can be large enough to push the decision boundary overboard, is, say, if a false positive is positioned to be in the half-plane of the negative classes. In this case too, linear separability cannot be verified, and again the convergence fails by defintion.</p><hr><p>I understand that this isn&rsquo;t really a &ldquo;proof&rdquo; proof. It is an explanation at best. False advertising. If you can find a simpler, or a more intuitive explanation, please leave a link in the comments.</p></p></div><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return}var b=document,a=b.createElement('script');a.async=!0,a.src='//brocasbrain.disqus.com/embed.js',a.setAttribute('data-timestamp',+new Date),(b.head||b.body).appendChild(a)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a><div class=page-footer><hr class=footer-divider><a class=tag href=/tags/math>#math</a>
<a class=tag href=/tags/perceptron>#perceptron</a>
<a class=tag href=/tags/machinelearning>#machinelearning</a></div><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></div><footer class=footer-mobile><div class=social-icons><a class=social-icon href=mailto:deshpande.jaidev@gmail.com target=_blank rel=noopener title=Email><svg width="28" height="28" viewBox="0 0 28 28" fill="#ababab" xmlns="https://www.w3.org/2000/svg" xmlns:xlink="https://www.w3.org/1999/xlink"><path d="M25.2794292 5.59128519 14 16.8707144 2.72057081 5.59128519C3.06733103 5.30237414 3.51336915 5.12857603 4 5.12857603H24c.4866308.0.932669000000001.173798110000001 1.2794292.46270916zM25.9956978 6.99633695C25.998551 7.04004843 26 7.08414302 26 7.12857603V20.871424C26 21.0798433 25.9681197 21.2808166 25.9089697 21.4697335l-7.1933342-7.1933342 7.2800623-7.28006235zM24.9498374 22.6319215C24.6672737 22.7846939 24.3437653 22.871424 24 22.871424H4C3.5268522 22.871424 3.09207889 22.7071233 2.74962118 22.432463l7.34540352-7.3454036 3.8897821 3.8897821L14.1878486 18.7737996l.0151933.0151933 3.4519334-3.4519335 7.2948621 7.2948621zM2.00810114 21.0526627C2.00273908 20.9929669 2 20.9325153 2 20.871424V7.12857603C2 7.08414302 2.00144896 7.04004843 2.00430222 6.99633695L9.03436454 14.0263993l-7.0262634 7.0262634z"/></svg></a><a class=social-icon href=https://twitter.com/jaidevd target=_blank rel=noopener title=Twitter><svg width="28" height="28" viewBox="0 0 28 28" fill="#ababab" xmlns="https://www.w3.org/2000/svg" xmlns:xlink="https://www.w3.org/1999/xlink"><path d="M8.991284 24.971612c10.189152.0 15.761088-8.441388 15.761088-15.761088C24.752372 8.970656 24.747512 8.731868 24.736496 8.494376 25.818008 7.712564 26.758256 6.737 27.5 5.62622c-.992628.440856-2.060748.738072-3.181248.871992 1.14372-.685584 2.02176-1.770768 2.435832-3.064176-1.070496.6345-2.25558 1.095984-3.517344 1.344492-1.010772-1.076652-2.450412-1.75014-4.043412-1.75014-3.059424.0-5.540292 2.480868-5.540292 5.539104.0.434808.0487079999999995.857412.14364 1.26306C9.19346 9.599108 5.11106 7.39472 2.3792 4.04294c-.476172.818424-.750168 1.769688-.750168 2.784132.0 1.921968.97794 3.61854 2.464992 4.61106C3.185528 11.41016 2.331788 11.160464 1.585184 10.745096 1.583888 10.768208 1.583888 10.791428 1.583888 10.815728c0 2.683152 1.909764 4.922856 4.4442 5.43078C5.562932 16.373084 5.07326 16.44134 4.56782 16.44134 4.210988 16.44134 3.863876 16.406024 3.526484 16.34144c.70524 2.200824 2.750112 3.802356 5.174928 3.8475-1.896264 1.485756-4.284576 2.37114-6.879924 2.37114C1.374476 22.56008.93362 22.534592.5 22.4834c2.451708 1.571076 5.362524 2.488212 8.491284 2.488212"/></svg></a><a class=social-icon href="https://www.youtube.com/playlist?list=PLllKLgiXxcqe3MlAk-6ZrQP82Dr5mgI0d" target=_blank rel=noopener title=YouTube><svg width="28" height="28" viewBox="0 0 28 28" fill="#ababab" xmlns="https://www.w3.org/2000/svg" xmlns:xlink="https://www.w3.org/1999/xlink"><path d="M25.9775568 20.4086648C25.6900568 21.4913352 24.8430398 22.343892 23.7673295 22.6332386 21.8177557 23.1590909 14 23.1590909 14 23.1590909s-7.81771307.0-9.76734375-.5258523C3.15704545 22.343892 2.30988068 21.4913352 2.02240483 20.4086648 1.5 18.4464062 1.5 14.3522727 1.5 14.3522727s0-4.0940767.52240483-6.05651986c.28747585-1.08254261 1.13464062-1.93509091 2.21025142-2.22456392C6.18228693 5.54545455 14 5.54545455 14 5.54545455s7.8177557.0 9.7673295.52573437c1.0757103.28947301 1.9227273 1.14202131 2.2102273 2.22456392C26.5 10.258196 26.5 14.3522727 26.5 14.3522727S26.5 18.4464062 25.9775568 20.4086648zm-14.534375-9.773537v7.434304l6.5340909-3.7173295-6.5340909-3.7169745z"/></svg></a><a class=social-icon href=https://github.com/jaidevd target=_blank rel=noopener title=GitHub><svg width="28" height="28" viewBox="0 0 28 28" fill="#ababab" xmlns="https://www.w3.org/2000/svg" xmlns:xlink="https://www.w3.org/1999/xlink"><path d="M13.9988029 1.32087331C6.82105037 1.32087331 1 7.14112562 1 14.3212723c0 5.7436386 3.72454649 10.6157955 8.89038951 12.3348169C10.5408085 26.7757983 10.7778323 26.374374 10.7778323 26.0296121 10.7778323 25.7215609 10.7666595 24.9035493 10.760275 23.8189856 7.14426471 24.6042767 6.38131925 22.0760223 6.38131925 22.0760223c-.59136253-1.5019491-1.44369072-1.9017772-1.44369072-1.9017772C3.75729765 19.3682044 5.02701126 19.3841656 5.02701126 19.3841656 6.33183953 19.4759425 7.01817121 20.7241085 7.01817121 20.7241085c1.15958133 1.9863716 3.04300319 1.4125664 3.78360289 1.0797753.1181129-.8395592.4540962-1.4125663.8251942-1.7373768C8.74038491 19.7385043 5.70536235 18.6228163 5.70536235 13.6413251c0-1.4189508.50676816-2.5801283 1.33834679-3.4883207C6.90963504 9.82420367 6.46351945 8.50181809 7.17139875 6.71256734c0 0 1.09094816-.34955032 3.57451115 1.33276037C11.78259 7.75642995 12.8950858 7.61277914 14.000399 7.60719272 15.1049142 7.61277914 16.2166119 7.75642995 17.2548881 8.04532771c2.4819669-1.68231069 3.571319-1.33276037 3.571319-1.33276037C21.5356825 8.50181809 21.0895669 9.82420367 20.9562909 10.1530044 21.7894656 11.0611968 22.2922435 12.2223743 22.2922435 13.6413251c0 4.9942601-3.039811 6.0931889-5.935173 6.4148071.4660671.401424200000001.8818564 1.194696.8818564 2.4077473C17.2389269 24.2012564 17.2229657 25.603448 17.2229657 26.0296121 17.2229657 26.3775663 17.4575954 26.7821827 18.116793 26.6552912 23.2786458 24.9322794 27 20.0633148 27 14.3212723 27 7.14112562 21.1789496 1.32087331 13.9988029 1.32087331"/></svg></a></div><div class=footer-mobile-links><p><a href=https://github.com/kimcc/hugo-theme-noteworthy target=_blank rel=noopener>Noteworthy theme</a></p><span class=divider-bar>|</span><p><a href=https://gohugo.io target=_blank rel=noopener>Built with Hugo</a></p></div><script src=http://jaidevd.com/js/main.min.c1aee25a817e9beb1f9c4afd9d62311227a7f5e46720e404dc1dda97281f47f2.js integrity="sha256-wa7iWoF+m+sfnEr9nWIxEien9eRnIOQE3B3alygfR/I=" crossorigin=anonymous></script></footer></body></html>