<!DOCTYPE html>

<html lang="en-us">
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="format-detection" content="telephone=no"/>

    <title>A Geometric Proof of the Perceptron Convergence Theorem | Jaidev&#39;s Blog</title>
    
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/manifest.json">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#FF3DB4">
    <meta name="theme-color" content="#ffffff">

    
    
    
    <link rel="stylesheet" href="https://jaidevd.com/css/main.min.0564082dc658ec59ca122dd2fd20110a2c5deb5ab5636ddec8b19222113d4fbd.css"/>

    
    
    

    
    
 
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-80129098-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
</head>

    <body>
        
<nav>
  <header>
    <div class="site-title">
        <a href="/">Jaidev&#39;s Blog</a>
    </div>  
</header>

  <div class="nav-menu">
  
    <a class="color-link nav-link" href="/about/">About Me</a>
  
    <a class="color-link nav-link" href="/archive/">Archive</a>
  
  <a class="color-link nav-link" href="https://jaidevd.com/index.xml" target="_blank" rel="noopener" type="application/rss+xml">RSS</a>
</div>
<footer class="footer">
	<div class="social-icons">
        
    <a class="social-icon" href="mailto:deshpande.jaidev@gmail.com" target="_blank" rel="noopener" title="Email">
        <svg width="28px" height="28px" viewBox="0 0 28 28" version="1.1" fill="#ABABAB" xmlns="https://www.w3.org/2000/svg" xmlns:xlink="https://www.w3.org/1999/xlink">
            <path d="M25.2794292,5.59128519 L14,16.8707144 L2.72057081,5.59128519 C3.06733103,5.30237414 3.51336915,5.12857603 4,5.12857603 L24,5.12857603 C24.4866308,5.12857603 24.932669,5.30237414 25.2794292,5.59128519 Z M25.9956978,6.99633695 C25.998551,7.04004843 26,7.08414302 26,7.12857603 L26,20.871424 C26,21.0798433 25.9681197,21.2808166 25.9089697,21.4697335 L18.7156355,14.2763993 L25.9956978,6.99633695 Z M24.9498374,22.6319215 C24.6672737,22.7846939 24.3437653,22.871424 24,22.871424 L4,22.871424 C3.5268522,22.871424 3.09207889,22.7071233 2.74962118,22.432463 L10.0950247,15.0870594 L13.9848068,18.9768415 L14.1878486,18.7737996 L14.2030419,18.7889929 L17.6549753,15.3370594 L24.9498374,22.6319215 Z M2.00810114,21.0526627 C2.00273908,20.9929669 2,20.9325153 2,20.871424 L2,7.12857603 C2,7.08414302 2.00144896,7.04004843 2.00430222,6.99633695 L9.03436454,14.0263993 L2.00810114,21.0526627 Z"></path>
        </svg>
    </a>
    

    

    
    <a class="social-icon" href="https://twitter.com/jaidevd" target="_blank" rel="noopener" title="Twitter">
        <svg width="28px" height="28px" viewBox="0 0 28 28" version="1.1" fill="#ABABAB" xmlns="https://www.w3.org/2000/svg" xmlns:xlink="https://www.w3.org/1999/xlink">
            <path d="M8.991284,24.971612 C19.180436,24.971612 24.752372,16.530224 24.752372,9.210524 C24.752372,8.970656 24.747512,8.731868 24.736496,8.494376 C25.818008,7.712564 26.758256,6.737 27.5,5.62622 C26.507372,6.067076 25.439252,6.364292 24.318752,6.498212 C25.462472,5.812628 26.340512,4.727444 26.754584,3.434036 C25.684088,4.068536 24.499004,4.53002 23.23724,4.778528 C22.226468,3.701876 20.786828,3.028388 19.193828,3.028388 C16.134404,3.028388 13.653536,5.509256 13.653536,8.567492 C13.653536,9.0023 13.702244,9.424904 13.797176,9.830552 C9.19346,9.599108 5.11106,7.39472 2.3792,4.04294 C1.903028,4.861364 1.629032,5.812628 1.629032,6.827072 C1.629032,8.74904 2.606972,10.445612 4.094024,11.438132 C3.185528,11.41016 2.331788,11.160464 1.585184,10.745096 C1.583888,10.768208 1.583888,10.791428 1.583888,10.815728 C1.583888,13.49888 3.493652,15.738584 6.028088,16.246508 C5.562932,16.373084 5.07326,16.44134 4.56782,16.44134 C4.210988,16.44134 3.863876,16.406024 3.526484,16.34144 C4.231724,18.542264 6.276596,20.143796 8.701412,20.18894 C6.805148,21.674696 4.416836,22.56008 1.821488,22.56008 C1.374476,22.56008 0.93362,22.534592 0.5,22.4834 C2.951708,24.054476 5.862524,24.971612 8.991284,24.971612"></path>
        </svg>
    </a>
    

    

    

    

    

    
    <a class="social-icon" href="https://www.youtube.com/playlist?list=PLllKLgiXxcqe3MlAk-6ZrQP82Dr5mgI0d" target="_blank" rel="noopener" title="YouTube">
        <svg width="28px" height="28px" viewBox="0 0 28 28" version="1.1" fill="#ABABAB" xmlns="https://www.w3.org/2000/svg" xmlns:xlink="https://www.w3.org/1999/xlink">
            <path d="M25.9775568,20.4086648 C25.6900568,21.4913352 24.8430398,22.343892 23.7673295,22.6332386 C21.8177557,23.1590909 14,23.1590909 14,23.1590909 C14,23.1590909 6.18228693,23.1590909 4.23265625,22.6332386 C3.15704545,22.343892 2.30988068,21.4913352 2.02240483,20.4086648 C1.5,18.4464062 1.5,14.3522727 1.5,14.3522727 C1.5,14.3522727 1.5,10.258196 2.02240483,8.29575284 C2.30988068,7.21321023 3.15704545,6.36066193 4.23265625,6.07118892 C6.18228693,5.54545455 14,5.54545455 14,5.54545455 C14,5.54545455 21.8177557,5.54545455 23.7673295,6.07118892 C24.8430398,6.36066193 25.6900568,7.21321023 25.9775568,8.29575284 C26.5,10.258196 26.5,14.3522727 26.5,14.3522727 C26.5,14.3522727 26.5,18.4464062 25.9775568,20.4086648 Z M11.4431818,10.6351278 L11.4431818,18.0694318 L17.9772727,14.3521023 L11.4431818,10.6351278 Z"></path>
        </svg>
    </a>
    

    

    

    

    

    

    

    

    

    

    

    
    
    
    <a class="social-icon" href="https://github.com/jaidevd" target="_blank" rel="noopener" title="GitHub">
        <svg width="28px" height="28px" viewBox="0 0 28 28" version="1.1" fill="#ABABAB" xmlns="https://www.w3.org/2000/svg" xmlns:xlink="https://www.w3.org/1999/xlink">
            <path d="M13.9988029,1.32087331 C6.82105037,1.32087331 1,7.14112562 1,14.3212723 C1,20.0649109 4.72454649,24.9370678 9.89038951,26.6560892 C10.5408085,26.7757983 10.7778323,26.374374 10.7778323,26.0296121 C10.7778323,25.7215609 10.7666595,24.9035493 10.760275,23.8189856 C7.14426471,24.6042767 6.38131925,22.0760223 6.38131925,22.0760223 C5.78995672,20.5740732 4.93762853,20.1742451 4.93762853,20.1742451 C3.75729765,19.3682044 5.02701126,19.3841656 5.02701126,19.3841656 C6.33183953,19.4759425 7.01817121,20.7241085 7.01817121,20.7241085 C8.17775254,22.7104801 10.0611744,22.1366749 10.8017741,21.8038838 C10.919887,20.9643246 11.2558703,20.3913175 11.6269683,20.066507 C8.74038491,19.7385043 5.70536235,18.6228163 5.70536235,13.6413251 C5.70536235,12.2223743 6.21213051,11.0611968 7.04370914,10.1530044 C6.90963504,9.82420367 6.46351945,8.50181809 7.17139875,6.71256734 C7.17139875,6.71256734 8.26234691,6.36301702 10.7459099,8.04532771 C11.78259,7.75642995 12.8950858,7.61277914 14.000399,7.60719272 C15.1049142,7.61277914 16.2166119,7.75642995 17.2548881,8.04532771 C19.736855,6.36301702 20.8262071,6.71256734 20.8262071,6.71256734 C21.5356825,8.50181809 21.0895669,9.82420367 20.9562909,10.1530044 C21.7894656,11.0611968 22.2922435,12.2223743 22.2922435,13.6413251 C22.2922435,18.6355852 19.2524325,19.734514 16.3570705,20.0561322 C16.8231376,20.4575564 17.2389269,21.2508282 17.2389269,22.4638795 C17.2389269,24.2012564 17.2229657,25.603448 17.2229657,26.0296121 C17.2229657,26.3775663 17.4575954,26.7821827 18.116793,26.6552912 C23.2786458,24.9322794 27,20.0633148 27,14.3212723 C27,7.14112562 21.1789496,1.32087331 13.9988029,1.32087331"></path>
        </svg>
    </a>
    

    
    
    

    

    

    

    

    

</div>




	<p><a href="https://github.com/kimcc/hugo-theme-noteworthy" target="_blank" rel="noopener">Noteworthy theme</a></p>
	<p><a href="https://gohugo.io" target="_blank" rel="noopener">Built with Hugo</a></p>

	<script src="https://jaidevd.com/js/main.min.fa5c2b23e07b5d9bfad267a52b4b24fdb053e6fb7524993383594926a3ac270c.js" integrity="sha256-+lwrI+B7XZv60melK0sk/bBT5vt1JJkzg1lJJqOsJww=" crossorigin="anonymous"></script>
</footer>
</nav>

        <div id="content" class="content-container">
        

<h1 class="post-title">A Geometric Proof of the Perceptron Convergence Theorem</h1>
    
    <time>July 6, 2016</time>
    
    <div>
        <p>
        <p>The last time I studied neural networks in detail was five years ago in college. I did touch upon backpropagation when Andrew Ng&rsquo;s machine learning MOOC was offered on Coursera for the first time, but beyond that I&rsquo;ve only dabbled with them through <a href="http://keras.io">keras</a>. Then recently, when I read about Coursera&rsquo;s imminent decision to pull down much of their freely available material (you can read a rant about it <a href="http://reachtarunhere.github.io/2016/06/11/Golden-Age-of-MOOCs-is-over-and-why-I-hate-Coursera/">here</a>), I went on a downloading spree (many thanks to the wonderful <a href="http://github.com/coursera-dl/coursera-dl">coursera-dl</a>). Of all the courses I downloaded, the one that caught my eye was Geoffrey Hinton&rsquo;s course on <em>Neural Networks for Machine Learning</em>. Because of that and the fact that there were some computer vision projects going on at work, I decided to dive right in.</p>
<p>Hinton&rsquo;s course is wonderful. He is funny, and unsurprisingly, very very insightful about the core concepts in neural networks. One of the signs of this is the fact that this course is not at all cluttered with too much mathematics, and can be traveresed by someone with only a working knowledge of calculus. One of his most insightful moments in the course is when he describes the Perceptron learning rule as simply as follows:</p>
<ul>
<li>If the perceptron makes no mistake, leave it alone.</li>
<li>If it predicts a false negative, add the input vector to the weight vector</li>
<li>If it predicts a false positive, subtract the input vector from the weight vector</li>
</ul>
<p>This is so simple, that a literal implementation of this can make train a perceptron reasonably well (as we shall see). There are of course, numerous heuristics required when applying it in production, but the training algorithm is just this simple. Now, the popularity of the perceptron is because it guarantees linear convergence, i.e. if a binary classification problem is linearly separable in the feature space, the perceptron will <em>always eventually</em> learn to correctly classify the input samples. An algebraic or analytical proof of this can be found anywhere, but relies almost always on the <a href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality">Cauchy-Schwarz inequality</a>. I thought that since the learning rule is so simple, then there must be a way to understand the convergence theorem using nothing more than the learning rule itself, and some simple data visualization. I think I&rsquo;ve found a reasonable explanation, which is what this post is broadly about. But first, let&rsquo;s see a simple demonstration of training a perceptron.</p>
<!-- TEASER_END -->
<h2 id="a-demo-perceptron">A Demo Perceptron</h2>
<p>Let&rsquo;s make some sample data which is linearly separable. Let&rsquo;s also initialize the weights for a perceptron. For simplicity, we shall do away with the bias, and make sure that the data is centered at the origin</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">import</span> <span style="color:#555">numpy</span> <span style="color:#000;font-weight:bold">as</span> <span style="color:#555">np</span>
<span style="color:#000;font-weight:bold">from</span> <span style="color:#555">sklearn.datasets</span> <span style="color:#000;font-weight:bold">import</span> make_blobs
<span style="color:#000;font-weight:bold">import</span> <span style="color:#555">matplotlib.pyplot</span> <span style="color:#000;font-weight:bold">as</span> <span style="color:#555">plt</span>

X, y <span style="color:#000;font-weight:bold">=</span> make_blobs(centers<span style="color:#000;font-weight:bold">=</span>[[<span style="color:#099">1</span>, <span style="color:#099">1</span>], [<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, <span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>]], cluster_std<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0.5</span>)
np<span style="color:#000;font-weight:bold">.</span>random<span style="color:#000;font-weight:bold">.</span>seed(<span style="color:#099">3</span>)
weights <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>array([[<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>], [<span style="color:#099">1</span>]], dtype<span style="color:#000;font-weight:bold">=</span><span style="color:#0086b3">float</span>)</code></pre></div>
<p>Let&rsquo;s also define a function that makes a scatterplot of the data and draws the decision boundary through it, based on the weights. The decision function we use is as follows</p>
<p>$$z = \sum_{i=1}^{N}w_{i}x_{i}$$</p>
<p>Where $N$ is the dimensionality, $x_{i}$ is the $i$th dimension of the input sample, and $w_{i}$ is the corresponding weight.</p>
<p>The prediction $y$ is 1 if $z \geq 0$ and 0 otherwise. Thus, the decision line in the feature space (consisting in this case of $x_{1}$ and $x_{2}$) is defined as follows:</p>
<p>$$ w_{1}x_{1} + w_{2}x_{2} = 0 $$</p>
<p><div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">plot_decision</span>(X, y, weights):
    <span style="color:#d14">&#34;&#34;&#34;Show the scatterplot of the data colored by the classes,
</span><span style="color:#d14">    draw the decision line based on the weights.&#34;&#34;&#34;</span>
    xx <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>linspace(X[:, <span style="color:#099">0</span>]<span style="color:#000;font-weight:bold">.</span>min(), X[:, <span style="color:#099">0</span>]<span style="color:#000;font-weight:bold">.</span>max())
    yy <span style="color:#000;font-weight:bold">=</span> <span style="color:#000;font-weight:bold">-</span> weights[<span style="color:#099">0</span>] <span style="color:#000;font-weight:bold">*</span> xx <span style="color:#000;font-weight:bold">/</span> weights[<span style="color:#099">1</span>]
    plt<span style="color:#000;font-weight:bold">.</span>figure(figsize<span style="color:#000;font-weight:bold">=</span>(<span style="color:#099">8</span>, <span style="color:#099">6</span>))
    plt<span style="color:#000;font-weight:bold">.</span>scatter(X[:, <span style="color:#099">0</span>], X[:, <span style="color:#099">1</span>], c<span style="color:#000;font-weight:bold">=</span>y)
    plt<span style="color:#000;font-weight:bold">.</span>plot(xx, yy)

plot_decision(X, y, weights)</code></pre></div>
<img src="/img/perc-1.png" alt=""></p>
<p>Note that I&rsquo;ve specifically chosen the weights such that the decision boundary ends up in the worst possible position. Now let&rsquo;s train the perceptron in batch mode on one epoch of the training set.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">prediction <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>dot(X, weights)<span style="color:#000;font-weight:bold">.</span>ravel()<span style="color:#000;font-weight:bold">.</span>astype(<span style="color:#0086b3">int</span>)
<span style="color:#998;font-style:italic"># indentify the misclassifications</span>
false_pos <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>logical_and(y <span style="color:#000;font-weight:bold">==</span> <span style="color:#099">0</span>, prediction <span style="color:#000;font-weight:bold">==</span> <span style="color:#099">1</span>)
false_neg <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>logical_and(y <span style="color:#000;font-weight:bold">==</span> <span style="color:#099">1</span>, prediction <span style="color:#000;font-weight:bold">==</span> <span style="color:#099">0</span>)
<span style="color:#998;font-style:italic"># add the false negatives</span>
weights <span style="color:#000;font-weight:bold">+=</span> X[false_neg, :]<span style="color:#000;font-weight:bold">.</span>sum(<span style="color:#099">0</span>)<span style="color:#000;font-weight:bold">.</span>reshape(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, <span style="color:#099">1</span>)
<span style="color:#998;font-style:italic"># subtract the false positives</span>
weights <span style="color:#000;font-weight:bold">-=</span> X[false_pos, :]<span style="color:#000;font-weight:bold">.</span>sum(<span style="color:#099">0</span>)<span style="color:#000;font-weight:bold">.</span>reshape(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, <span style="color:#099">1</span>)

<span style="color:#998;font-style:italic"># plot the data again</span>
plot_decision(X, y, weights)</code></pre></div>
<p><img src="/img/perc-2.png" alt=""></p>
<p>It can be seen that one epoch or a hundred iterations (maybe less) are enough to train the perceptron. Now let&rsquo;s do the same in online mode. This time, we will store the weights updated at each iteration so we can have a look at them later.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#998;font-style:italic"># reset the weights</span>
weights <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>array([[<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>], [<span style="color:#099">1</span>]], dtype<span style="color:#000;font-weight:bold">=</span><span style="color:#0086b3">float</span>)
updated_weights <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>zeros((<span style="color:#099">100</span>, <span style="color:#099">2</span>))
<span style="color:#000;font-weight:bold">for</span> i <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(X<span style="color:#000;font-weight:bold">.</span>shape[<span style="color:#099">0</span>]):
    updated_weights[i, :] <span style="color:#000;font-weight:bold">=</span> weights<span style="color:#000;font-weight:bold">.</span>ravel()  <span style="color:#998;font-style:italic"># store the weights</span>
    training_sample <span style="color:#000;font-weight:bold">=</span> X[i, :]
    target <span style="color:#000;font-weight:bold">=</span> y[i]
    prediction <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>dot(training_sample, weights)<span style="color:#000;font-weight:bold">.</span>ravel()<span style="color:#000;font-weight:bold">.</span>astype(<span style="color:#0086b3">int</span>)
    <span style="color:#000;font-weight:bold">if</span> target <span style="color:#000;font-weight:bold">!=</span> prediction:
        <span style="color:#000;font-weight:bold">if</span> target <span style="color:#000;font-weight:bold">==</span> <span style="color:#099">0</span> <span style="color:#000;font-weight:bold">and</span> prediction <span style="color:#000;font-weight:bold">==</span> <span style="color:#099">1</span>:  <span style="color:#998;font-style:italic"># false positive</span>
            weights <span style="color:#000;font-weight:bold">-=</span> training_sample<span style="color:#000;font-weight:bold">.</span>reshape(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, <span style="color:#099">1</span>)
        <span style="color:#000;font-weight:bold">elif</span> target <span style="color:#000;font-weight:bold">==</span> <span style="color:#099">1</span> <span style="color:#000;font-weight:bold">and</span> prediction <span style="color:#000;font-weight:bold">==</span> <span style="color:#099">0</span>:  <span style="color:#998;font-style:italic"># false negative</span>
            weights <span style="color:#000;font-weight:bold">+=</span> training_sample<span style="color:#000;font-weight:bold">.</span>reshape(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, <span style="color:#099">1</span>)</code></pre></div>
<p>The two snippets above achieve exactly the same effect, except that the first is faster since it is vectorized. Now that we have the weight updates for every training sample, let&rsquo;s plot the decision boundary for a few of them.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">sample_ix <span style="color:#000;font-weight:bold">=</span> [<span style="color:#099">1</span>, <span style="color:#099">20</span>, <span style="color:#099">30</span>]
xx <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>linspace(X[:, <span style="color:#099">0</span>]<span style="color:#000;font-weight:bold">.</span>min(), X[:, <span style="color:#099">0</span>]<span style="color:#000;font-weight:bold">.</span>max(), <span style="color:#099">100</span>)
plt<span style="color:#000;font-weight:bold">.</span>figure(figsize<span style="color:#000;font-weight:bold">=</span>(<span style="color:#099">8</span>, <span style="color:#099">6</span>))
plt<span style="color:#000;font-weight:bold">.</span>scatter(X[:, <span style="color:#099">0</span>], X[:, <span style="color:#099">1</span>], c<span style="color:#000;font-weight:bold">=</span>y)

<span style="color:#000;font-weight:bold">for</span> ix <span style="color:#000;font-weight:bold">in</span> sample_ix:
    wghts <span style="color:#000;font-weight:bold">=</span> updated_weights[ix, :]
    yy <span style="color:#000;font-weight:bold">=</span> <span style="color:#000;font-weight:bold">-</span> wghts[<span style="color:#099">0</span>] <span style="color:#000;font-weight:bold">*</span> xx <span style="color:#000;font-weight:bold">/</span> wghts[<span style="color:#099">1</span>]
    plt<span style="color:#000;font-weight:bold">.</span>plot(xx, yy, label<span style="color:#000;font-weight:bold">=</span><span style="color:#d14">&#34;{}th sample&#34;</span><span style="color:#000;font-weight:bold">.</span>format(ix))
plt<span style="color:#000;font-weight:bold">.</span>legend()</code></pre></div>
<p><img src="/img/perc-3.png" alt=""></p>
<p>As we move from the first to the twentieth and then to the thirtieth sample, we can see that the decision boundary is rotating counter clockwise. By the thirtieth sample, it has found the gap that separates the two blobs, so it stops making mistakes, and therefore stops learning. Now let&rsquo;s try and see why this will <em>always</em> happen, as long as there is such a gap to be found, irrespective of the topology of the data or the number of points in the dataset.</p>
<h2 id="the-proof">The Proof</h2>
<p>We shall continue, for the sake of simplicity, our assumption of the data being centered at the origin and therefore not needing a bias term in the learning. For better visualization, we will also impose a restriction on the weight vector - that it needs to have a unit norm. Note that the magnitude of the weight vector does not matter, since it is only its gradient that actually affects the predictions, and therefore we can comfortably impose this restriction. Moreover, I&rsquo;m insisting on this restriction only to make the plots look pretty.</p>
<p>Consider the following dataset:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">X <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>array([[<span style="color:#000;font-weight:bold">-</span><span style="color:#099">0.5</span>, <span style="color:#099">1</span>],
              [<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, <span style="color:#000;font-weight:bold">-</span><span style="color:#099">0.2</span>],
              [<span style="color:#099">0.5</span>, <span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>],
              [<span style="color:#099">1</span>, <span style="color:#000;font-weight:bold">-</span><span style="color:#099">0.2</span>]])
y <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>array([<span style="color:#099">1</span>, <span style="color:#099">1</span>, <span style="color:#099">0</span>, <span style="color:#099">0</span>])
weights <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>array([<span style="color:#099">0.707</span>, <span style="color:#099">0.707</span>])</code></pre></div>
<p>Now, we arrange the training samples and the weights in a cartesian plane, by their position vectors.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">cls_color <span style="color:#000;font-weight:bold">=</span> [<span style="color:#d14">&#34;r&#34;</span>, <span style="color:#d14">&#34;g&#34;</span>]
plt<span style="color:#000;font-weight:bold">.</span>figure(figsize<span style="color:#000;font-weight:bold">=</span>(<span style="color:#099">6</span>, <span style="color:#099">6</span>))
ax <span style="color:#000;font-weight:bold">=</span> plt<span style="color:#000;font-weight:bold">.</span>axes()
ax<span style="color:#000;font-weight:bold">.</span>set_xlim(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">2</span>, <span style="color:#099">2</span>)
ax<span style="color:#000;font-weight:bold">.</span>set_ylim(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">2</span>, <span style="color:#099">2</span>)
<span style="color:#000;font-weight:bold">for</span> i <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(<span style="color:#099">4</span>):
    ax<span style="color:#000;font-weight:bold">.</span>arrow(<span style="color:#099">0</span>, <span style="color:#099">0</span>, X[i, <span style="color:#099">0</span>], X[i, <span style="color:#099">1</span>], fc<span style="color:#000;font-weight:bold">=</span>cls_color[y[i]], ec<span style="color:#000;font-weight:bold">=</span>cls_color[y[i]],
    	     head_length<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0.1</span>, head_width<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0.1</span>)
ax<span style="color:#000;font-weight:bold">.</span>arrow(<span style="color:#099">0</span>, <span style="color:#099">0</span>, weights[<span style="color:#099">0</span>], weights[<span style="color:#099">1</span>], fc<span style="color:#000;font-weight:bold">=</span><span style="color:#d14">&#34;k&#34;</span>, ec<span style="color:#000;font-weight:bold">=</span><span style="color:#d14">&#34;k&#34;</span>, head_width<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0.1</span>,
         head_length<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0.1</span>)

<span style="color:#998;font-style:italic"># decision boundary:</span>
xx <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>linspace(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">2</span>, <span style="color:#099">2</span>, <span style="color:#099">100</span>)
yy <span style="color:#000;font-weight:bold">=</span> <span style="color:#000;font-weight:bold">-</span>weights[<span style="color:#099">0</span>] <span style="color:#000;font-weight:bold">*</span> xx <span style="color:#000;font-weight:bold">/</span> weights[<span style="color:#099">1</span>]
ax<span style="color:#000;font-weight:bold">.</span>plot(xx, yy, <span style="color:#d14">&#34;k--&#34;</span>)</code></pre></div>
<p><img src="/img/perc-4.png" alt=""></p>
<p>The green arrows denote the true positives, the red ones denote the true negatives, the black arrow is the weight vector and the black dashed line denotes the decision boundary. Our hypothesis based on this decision boundary is equivalent to saying that any point in the upper right triangle would be assigned a positive prediction, and any point in the lower left triangle would be assigned a negative prediction. In the given dataset, this hypothesis clearly makes two mistakes - one false positive and one false negative.</p>
<p>Analogous to the previous example, we know that the perceptron will converge if the weight vector (and hence the decision boundary) keeps rotating. Let&rsquo;s assume that it rotates counter-clockwise. Proving the convergence, then, can be reduced into proving the following parts:</p>
<ul>
<li><strong>Part A</strong>: No sample will ever make the weight vector rotate clockwise.</li>
<li><strong>Part B</strong>: The weight vector will not rotate enough to make the decision boundary jump <em>across</em> the gap between the two classes.</li>
</ul>
<p>Let us apply the learning rule to this perceptron, and see how we can prove both Part A and Part B. Since there is one false positive and one false negative, the training simply consists of adding the false negative to the weight vector and subtracting the false positive from it.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">fig, (ax1, ax2) <span style="color:#000;font-weight:bold">=</span> plt<span style="color:#000;font-weight:bold">.</span>subplots(nrows<span style="color:#000;font-weight:bold">=</span><span style="color:#099">1</span>, ncols<span style="color:#000;font-weight:bold">=</span><span style="color:#099">2</span>, figsize<span style="color:#000;font-weight:bold">=</span>(<span style="color:#099">16</span>, <span style="color:#099">8</span>))

ax1<span style="color:#000;font-weight:bold">.</span>axis([<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1.5</span>, <span style="color:#099">1.5</span>, <span style="color:#000;font-weight:bold">-</span><span style="color:#099">1.5</span>, <span style="color:#099">1.5</span>])
ax2<span style="color:#000;font-weight:bold">.</span>axis([<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1.5</span>, <span style="color:#099">1.5</span>, <span style="color:#000;font-weight:bold">-</span><span style="color:#099">1.5</span>, <span style="color:#099">1.5</span>])
<span style="color:#998;font-style:italic"># original weights</span>
weights <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>array([<span style="color:#099">0.707</span>, <span style="color:#099">0.707</span>])

<span style="color:#998;font-style:italic"># adding the false negative and normalizing</span>
weights <span style="color:#000;font-weight:bold">+=</span> [<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, <span style="color:#000;font-weight:bold">-</span><span style="color:#099">0.2</span>]
weights <span style="color:#000;font-weight:bold">=</span> weights <span style="color:#000;font-weight:bold">/</span> np<span style="color:#000;font-weight:bold">.</span>linalg<span style="color:#000;font-weight:bold">.</span>norm(weights)
<span style="color:#000;font-weight:bold">for</span> i <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(<span style="color:#099">4</span>):
    ax1<span style="color:#000;font-weight:bold">.</span>arrow(<span style="color:#099">0</span>, <span style="color:#099">0</span>, X[i, <span style="color:#099">0</span>], X[i, <span style="color:#099">1</span>], fc<span style="color:#000;font-weight:bold">=</span>cls_color[y[i]],
    	      ec<span style="color:#000;font-weight:bold">=</span>cls_color[y[i]], head_length<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0.1</span>,
              head_width<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0.1</span>)
ax1<span style="color:#000;font-weight:bold">.</span>arrow(<span style="color:#099">0</span>, <span style="color:#099">0</span>, weights[<span style="color:#099">0</span>], weights[<span style="color:#099">1</span>], fc<span style="color:#000;font-weight:bold">=</span><span style="color:#d14">&#34;k&#34;</span>, ec<span style="color:#000;font-weight:bold">=</span><span style="color:#d14">&#34;k&#34;</span>,
 	  head_width<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0.1</span>, head_length<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0.1</span>)
ax1<span style="color:#000;font-weight:bold">.</span>set_title(<span style="color:#d14">&#34;After adding false -ve&#34;</span>)

<span style="color:#998;font-style:italic"># subtracting the false positive and normalizing</span>
weights <span style="color:#000;font-weight:bold">-=</span> [<span style="color:#099">1</span>, <span style="color:#000;font-weight:bold">-</span><span style="color:#099">0.2</span>]
weights <span style="color:#000;font-weight:bold">=</span> weights <span style="color:#000;font-weight:bold">/</span> np<span style="color:#000;font-weight:bold">.</span>linalg<span style="color:#000;font-weight:bold">.</span>norm(weights)
<span style="color:#000;font-weight:bold">for</span> i <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(<span style="color:#099">4</span>):
    ax2<span style="color:#000;font-weight:bold">.</span>arrow(<span style="color:#099">0</span>, <span style="color:#099">0</span>, X[i, <span style="color:#099">0</span>], X[i, <span style="color:#099">1</span>], fc<span style="color:#000;font-weight:bold">=</span>cls_color[y[i]], ec<span style="color:#000;font-weight:bold">=</span>cls_color[y[i]],
              head_length<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0.1</span>,
              head_width<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0.1</span>)
ax2<span style="color:#000;font-weight:bold">.</span>arrow(<span style="color:#099">0</span>, <span style="color:#099">0</span>, weights[<span style="color:#099">0</span>], weights[<span style="color:#099">1</span>], fc<span style="color:#000;font-weight:bold">=</span><span style="color:#d14">&#34;k&#34;</span>, ec<span style="color:#000;font-weight:bold">=</span><span style="color:#d14">&#34;k&#34;</span>,
	  head_width<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0.1</span>, head_length<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0.1</span>)
ax2<span style="color:#000;font-weight:bold">.</span>set_title(<span style="color:#d14">&#34;After adding false -ve and subtracting false +ve&#34;</span>)</code></pre></div>
<p><img src="/img/perc-5.png" alt=""></p>
<h2 id="part-a">Part A</h2>
<p>Suppose a false negative makes and angle $\theta_{n}$ with the weight vector. Since the decision boundary is orothogonal to the weight vector, no matter which iteration of learning the perceptron is, every possible $\theta_{n}$ will be such that $\frac{\pi}{2} \leq \theta_{n} \leq \frac{3\pi}{2}$. In order to make the weight vector rotate clockwise, a vector which makes an angle of $\leq \frac{\pi}{2}$ must be added to it. But because of the orthogonality between the weight vector and the decision boundary, no false negative can have this property! Thus, there will never exist a false negative that makes the weight vector rotate clockwise.</p>
<p>In the same way, suppose a false positive makes an angle $\theta_{p}$ with the weight vector. Again, every possible $\theta_{p}$ will be such that $-\frac{\pi}{2} \leq \theta_{p} \leq \frac{\pi}{2}$. Here, too, there can&rsquo;t exist a vector which when subtracted from the weight vector will make it rotate clockwise. Thus, there will never exist a false positive that makes the weight vector rotate clockwise.</p>
<p>In the context of the example, the only way that the weight vector could rotate clockwise was if there was a green vector sticking between the two red vectors, or vice versa. In both these cases, linear separability would not be possible (remember that we are assuming no bias), and hence the convergence would fail by definition.</p>
<h2 id="part-b">Part B</h2>
<p>Recall that in order to make corrections, what we are adding to (or subtracting from) the weights are the sample vectors themselves - not some linear function of them. In other words, whenever the perceptron learns, it learns not only to be right, but to be right by a very specific margin. This margin is nothing but the magnitude of the last mistake it made! Now, the only way that this margin can be large enough to push the decision boundary overboard, is, say, if a false positive is positioned to be in the half-plane of the negative classes. In this case too, linear separability cannot be verified, and again the convergence fails by defintion.</p>
<hr>
<p>I understand that this isn&rsquo;t really a &ldquo;proof&rdquo; proof. It is an explanation at best. False advertising. If you can find a simpler, or a more intuitive explanation, please leave a link in the comments.</p>

        </p>
    </div>
    <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "brocasbrain" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    

    <div class="page-footer">
        
        <hr class="footer-divider">
        
            <a class="tag" href="/tags/math">#math</a>
        
            <a class="tag" href="/tags/perceptron">#perceptron</a>
        
            <a class="tag" href="/tags/machinelearning">#machinelearning</a>
        
      
    </div>


	


<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


        </div>
        <footer class="footer-mobile">
	<div class="social-icons">
        
    <a class="social-icon" href="mailto:deshpande.jaidev@gmail.com" target="_blank" rel="noopener" title="Email">
        <svg width="28px" height="28px" viewBox="0 0 28 28" version="1.1" fill="#ABABAB" xmlns="https://www.w3.org/2000/svg" xmlns:xlink="https://www.w3.org/1999/xlink">
            <path d="M25.2794292,5.59128519 L14,16.8707144 L2.72057081,5.59128519 C3.06733103,5.30237414 3.51336915,5.12857603 4,5.12857603 L24,5.12857603 C24.4866308,5.12857603 24.932669,5.30237414 25.2794292,5.59128519 Z M25.9956978,6.99633695 C25.998551,7.04004843 26,7.08414302 26,7.12857603 L26,20.871424 C26,21.0798433 25.9681197,21.2808166 25.9089697,21.4697335 L18.7156355,14.2763993 L25.9956978,6.99633695 Z M24.9498374,22.6319215 C24.6672737,22.7846939 24.3437653,22.871424 24,22.871424 L4,22.871424 C3.5268522,22.871424 3.09207889,22.7071233 2.74962118,22.432463 L10.0950247,15.0870594 L13.9848068,18.9768415 L14.1878486,18.7737996 L14.2030419,18.7889929 L17.6549753,15.3370594 L24.9498374,22.6319215 Z M2.00810114,21.0526627 C2.00273908,20.9929669 2,20.9325153 2,20.871424 L2,7.12857603 C2,7.08414302 2.00144896,7.04004843 2.00430222,6.99633695 L9.03436454,14.0263993 L2.00810114,21.0526627 Z"></path>
        </svg>
    </a>
    

    

    
    <a class="social-icon" href="https://twitter.com/jaidevd" target="_blank" rel="noopener" title="Twitter">
        <svg width="28px" height="28px" viewBox="0 0 28 28" version="1.1" fill="#ABABAB" xmlns="https://www.w3.org/2000/svg" xmlns:xlink="https://www.w3.org/1999/xlink">
            <path d="M8.991284,24.971612 C19.180436,24.971612 24.752372,16.530224 24.752372,9.210524 C24.752372,8.970656 24.747512,8.731868 24.736496,8.494376 C25.818008,7.712564 26.758256,6.737 27.5,5.62622 C26.507372,6.067076 25.439252,6.364292 24.318752,6.498212 C25.462472,5.812628 26.340512,4.727444 26.754584,3.434036 C25.684088,4.068536 24.499004,4.53002 23.23724,4.778528 C22.226468,3.701876 20.786828,3.028388 19.193828,3.028388 C16.134404,3.028388 13.653536,5.509256 13.653536,8.567492 C13.653536,9.0023 13.702244,9.424904 13.797176,9.830552 C9.19346,9.599108 5.11106,7.39472 2.3792,4.04294 C1.903028,4.861364 1.629032,5.812628 1.629032,6.827072 C1.629032,8.74904 2.606972,10.445612 4.094024,11.438132 C3.185528,11.41016 2.331788,11.160464 1.585184,10.745096 C1.583888,10.768208 1.583888,10.791428 1.583888,10.815728 C1.583888,13.49888 3.493652,15.738584 6.028088,16.246508 C5.562932,16.373084 5.07326,16.44134 4.56782,16.44134 C4.210988,16.44134 3.863876,16.406024 3.526484,16.34144 C4.231724,18.542264 6.276596,20.143796 8.701412,20.18894 C6.805148,21.674696 4.416836,22.56008 1.821488,22.56008 C1.374476,22.56008 0.93362,22.534592 0.5,22.4834 C2.951708,24.054476 5.862524,24.971612 8.991284,24.971612"></path>
        </svg>
    </a>
    

    

    

    

    

    
    <a class="social-icon" href="https://www.youtube.com/playlist?list=PLllKLgiXxcqe3MlAk-6ZrQP82Dr5mgI0d" target="_blank" rel="noopener" title="YouTube">
        <svg width="28px" height="28px" viewBox="0 0 28 28" version="1.1" fill="#ABABAB" xmlns="https://www.w3.org/2000/svg" xmlns:xlink="https://www.w3.org/1999/xlink">
            <path d="M25.9775568,20.4086648 C25.6900568,21.4913352 24.8430398,22.343892 23.7673295,22.6332386 C21.8177557,23.1590909 14,23.1590909 14,23.1590909 C14,23.1590909 6.18228693,23.1590909 4.23265625,22.6332386 C3.15704545,22.343892 2.30988068,21.4913352 2.02240483,20.4086648 C1.5,18.4464062 1.5,14.3522727 1.5,14.3522727 C1.5,14.3522727 1.5,10.258196 2.02240483,8.29575284 C2.30988068,7.21321023 3.15704545,6.36066193 4.23265625,6.07118892 C6.18228693,5.54545455 14,5.54545455 14,5.54545455 C14,5.54545455 21.8177557,5.54545455 23.7673295,6.07118892 C24.8430398,6.36066193 25.6900568,7.21321023 25.9775568,8.29575284 C26.5,10.258196 26.5,14.3522727 26.5,14.3522727 C26.5,14.3522727 26.5,18.4464062 25.9775568,20.4086648 Z M11.4431818,10.6351278 L11.4431818,18.0694318 L17.9772727,14.3521023 L11.4431818,10.6351278 Z"></path>
        </svg>
    </a>
    

    

    

    

    

    

    

    

    

    

    

    
    
    
    <a class="social-icon" href="https://github.com/jaidevd" target="_blank" rel="noopener" title="GitHub">
        <svg width="28px" height="28px" viewBox="0 0 28 28" version="1.1" fill="#ABABAB" xmlns="https://www.w3.org/2000/svg" xmlns:xlink="https://www.w3.org/1999/xlink">
            <path d="M13.9988029,1.32087331 C6.82105037,1.32087331 1,7.14112562 1,14.3212723 C1,20.0649109 4.72454649,24.9370678 9.89038951,26.6560892 C10.5408085,26.7757983 10.7778323,26.374374 10.7778323,26.0296121 C10.7778323,25.7215609 10.7666595,24.9035493 10.760275,23.8189856 C7.14426471,24.6042767 6.38131925,22.0760223 6.38131925,22.0760223 C5.78995672,20.5740732 4.93762853,20.1742451 4.93762853,20.1742451 C3.75729765,19.3682044 5.02701126,19.3841656 5.02701126,19.3841656 C6.33183953,19.4759425 7.01817121,20.7241085 7.01817121,20.7241085 C8.17775254,22.7104801 10.0611744,22.1366749 10.8017741,21.8038838 C10.919887,20.9643246 11.2558703,20.3913175 11.6269683,20.066507 C8.74038491,19.7385043 5.70536235,18.6228163 5.70536235,13.6413251 C5.70536235,12.2223743 6.21213051,11.0611968 7.04370914,10.1530044 C6.90963504,9.82420367 6.46351945,8.50181809 7.17139875,6.71256734 C7.17139875,6.71256734 8.26234691,6.36301702 10.7459099,8.04532771 C11.78259,7.75642995 12.8950858,7.61277914 14.000399,7.60719272 C15.1049142,7.61277914 16.2166119,7.75642995 17.2548881,8.04532771 C19.736855,6.36301702 20.8262071,6.71256734 20.8262071,6.71256734 C21.5356825,8.50181809 21.0895669,9.82420367 20.9562909,10.1530044 C21.7894656,11.0611968 22.2922435,12.2223743 22.2922435,13.6413251 C22.2922435,18.6355852 19.2524325,19.734514 16.3570705,20.0561322 C16.8231376,20.4575564 17.2389269,21.2508282 17.2389269,22.4638795 C17.2389269,24.2012564 17.2229657,25.603448 17.2229657,26.0296121 C17.2229657,26.3775663 17.4575954,26.7821827 18.116793,26.6552912 C23.2786458,24.9322794 27,20.0633148 27,14.3212723 C27,7.14112562 21.1789496,1.32087331 13.9988029,1.32087331"></path>
        </svg>
    </a>
    

    
    
    

    

    

    

    

    

</div>




	<div class="footer-mobile-links">
		<p><a href="https://github.com/kimcc/hugo-theme-noteworthy" target="_blank" rel="noopener">Noteworthy theme</a></p>
		<span class="divider-bar">|</span>
		<p><a href="https://gohugo.io" target="_blank" rel="noopener">Built with Hugo</a></p>
	</div>

	<script src="https://jaidevd.com/js/main.min.fa5c2b23e07b5d9bfad267a52b4b24fdb053e6fb7524993383594926a3ac270c.js" integrity="sha256-+lwrI+B7XZv60melK0sk/bBT5vt1JJkzg1lJJqOsJww=" crossorigin="anonymous"></script>
</footer>
    </body>
</html>
